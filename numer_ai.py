# -*- coding: utf-8 -*-
"""numer ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YWIZrrD2CJuvqR2HDcQMi2w2izHMWOyf
"""

import os
import json
import pandas as pd
import numpy as np
import lightgbm as lgb
from scipy.stats import rankdata
import numerapi

# -------------------------------
# Numerai API setup
# -------------------------------
napi = numerapi.NumerAPI(
    public_id="GR6CSQYNJ5LGLFEOGI6ACBXXKZ4VLZ7Q",
    secret_key="ZXJPQ4GI42ACW56X5JOSKXFABLZCXRD7FBC5WXLHZ6SVMFUGAP2CBNQKUJJKCT4Y"
)

# Replace with your actual Numerai model ID (copy from Numerai dashboard)
MODEL_ID = "paste-your-model-id-here"

# -------------------------------
# Paths to local files
# -------------------------------
features_path = r"C:\Users\velam\Downloads\features.json"
train_path = r"C:\Users\velam\Downloads\train.parquet"

# -------------------------------
# Phase 1: Load Features
# -------------------------------
with open(features_path, "r") as f:
    features_metadata = json.load(f)

# Small feature set, first 20 features
small_features = features_metadata["feature_sets"]["small"]
selected_features = small_features[:20]
print(f"Selected {len(selected_features)} features: {selected_features}")

# -------------------------------
# Phase 2: Load Train Data
# -------------------------------
train_data = pd.read_parquet(train_path, columns=selected_features + ["era", "target"])
# Keep only eras 1-10
train_data = train_data[train_data["era"].isin([f"{i:04d}" for i in range(1, 11)])]

train_data[selected_features] = train_data[selected_features].astype(np.float32)
train_data["target"] = train_data["target"].astype(np.float32)

print(f"Train data shape: {train_data.shape}")

# -------------------------------
# Phase 3: Train LightGBM Model
# -------------------------------
params = {
    "objective": "regression",
    "metric": "mse",
    "boosting_type": "gbdt",
    "num_leaves": 31,
    "learning_rate": 0.1,
    "feature_fraction": 0.9,
    "bagging_fraction": 0.8,
    "bagging_freq": 5,
    "verbose": -1,
    "max_depth": 5
}

X_train = train_data[selected_features]
y_train = train_data["target"]
train_dataset = lgb.Dataset(X_train, label=y_train)
model = lgb.train(params, train_dataset, num_boost_round=100)
print("Model trained successfully.")

# -------------------------------
# Phase 4: Load Current Round Live Data
# -------------------------------
current_round = napi.get_current_round()
live_path = fr"C:\Users\velam\Downloads\live_{current_round}.parquet"
live_data = pd.read_parquet(live_path)

# Check for 'id' column
if "id" not in live_data.columns:
    # maybe id is in index
    if live_data.index.name:
        live_data = live_data.reset_index()
        print("Reset index, new columns:", live_data.columns)
    else:
        raise ValueError("No 'id' column found in live data!")

# Filter features and cast
X_live = live_data[selected_features].astype(np.float32)

# Predict
predictions = model.predict(X_live)

# Rank-normalize predictions
normalized_predictions = rankdata(predictions, method="average") / len(predictions)

# -------------------------------
# Phase 5: Save Submission CSV
# -------------------------------
submission = pd.DataFrame({
    "id": live_data["id"],
    "prediction": normalized_predictions
})

submission_file = fr"C:\Users\velam\Downloads\numerai_submission_{current_round}.csv"
submission.to_csv(submission_file, index=False)
print(f"Submission CSV saved: {submission_file}")

# -------------------------------
# Phase 6: Upload to Numerai
# -------------------------------
napi.upload_predictions(submission_file, model_id=MODEL_ID)
print("Submission uploaded successfully!")